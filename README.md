# AutoGAN

Paper:  
* [AutoGAN: Neural Architecture Search for Generative Adversarial Networks](https://arxiv.org/abs/1908.03835). 
  
  
Data: 
* [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) 
* [STL-10](http://ai.stanford.edu/~acoates/stl10/)

## References:
1. [AutoGAN code implementation](https://github.com/TAMU-VITA/AutoGAN)
2. [AdversarialNAS: Adversarial Neural Architecture Search for GANs](https://arxiv.org/abs/1912.02037)
3. [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)
4. [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)
5. [NAT: Neural Architecture Transformer for Accurate and Compact Architectures](https://arxiv.org/abs/1910.14488)
6. [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)
7. [FID Score - GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500): [Code](https://github.com/bioinf-jku/TTUR)

## Review

AutoGAN is published at ICCV 2019. As per the guidlines in [A Step Toward Quantifying Independently Reproducible Machine Learning Research](https://arxiv.org/abs/1909.06674), the features of the paper are listed below.

| Features | Comments|
| -- | --|
|Year Published| 2019|
|Year First Attempted| - |
|Venue Type| Conference |
|Rigor vs Empirical| Empirical |
|Has Appendix| No |
|Looks Intimidating| No |
|Readability| Good |
|Algorithm Difficulty| High |
|Pseudo Code| Yes |
|Primary Topic| NAS, GAN |
|Exemplar Problem| No |
|Compute Specified| No |
|Hyperparameters Specified| Partial |
|Compute Needed| Yes |
|Authors Reply| na |
|Code Available| Yes |
|Pages|11|
|Publication Venue| ICCV|
|Number of References| 73 |
|Number Equations| 3 |
|Number Proofs| 0 |
|Number Tables| 2 |
|Number Graph/Plots| 4 |
|Number Other Figures| 2 |
|Conceptualization Figures| 4 |
|Number of Authors| 4 |

The paper should be easily reproducable with an exception on computation costs.

## Timeline
Weekly dealine is as listed below starting from the week of April 20th.  
  
Week0: Literature Review and Project Proposal.  
Week1: Setup the evaluation pipeline. Download the pre-trained code and test the code.   
Week2: Training and performance evaluation.   
Week3: Improve the performance and explore applications.   
Week4: Future work  ...   


